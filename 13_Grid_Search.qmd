---
title: "13. Grid Search"
format: html
date: last-modified
execute:
    cache: true
code-fold: true
toc: true
---

```{r}
#| label: code

library(tidymodels)

```

## 13.1. Regular and Non-Regular Grids

Using a multi-layer perceptron (MLP) model (AKA single layer artificial NN) as an example, this section demonstrates how to set up a grid search for hyperparameter tuning using the `tidymodels` framework in R.

```{r}
#| label: setup

mlp_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_engine("nnet", trace = 0) %>%
  set_mode("classification")
```

Checking default parameter values for the MLP model specification.

```{r}
#| label: extract-param-defaults

mlp_param <- extract_parameter_set_dials(mlp_spec)

mlp_param %>% extract_parameter_dials("hidden_units")

mlp_param %>% extract_parameter_dials("penalty")

mlp_param %>% extract_parameter_dials("epochs")
```

Regular grid search over three hyperparameters: `hidden_units`, `penalty`, and `epochs`.

```{r}
#| label: reg-grid-crossing

crossing(
  hidden_units = 1:3,
  penalty = c(0.0, 0.1),
  epochs = c(100, 200)
)
```

**dials** package has *grid_()* functions.

```{r}
#| label: grid-regular

# same levels for all parameters
grid_regular(mlp_param, levels = 2)

# custom levels for each parameter
mlp_param %>%
  grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2))
```

Fractional factorial designs (see Box, Hunter, and Hunter 2005) can be used to skip some values.

Can generate grids irregularly spaced values using `grid_random()`. 

```{r}
#| label: grid-random

set.seed(1301)
mlp_param %>%
  grid_random(size = 1000) %>% # 'size' is the number of combinations
  summary()
```

Random can result in a lot of overlap, especially for small-to-medium sized grids. 

```{r}
#| label: random-design

library(ggforce)

set.seed(1302)
mlp_param %>%
  # The 'original = FALSE' option keeps penalty in log10 units
  grid_random(size = 20, original = FALSE) %>%
  ggplot(aes(x = .panel_x, y = .panel_y)) +
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) +
  labs(title = "Random design with 20 candidates")
```

Better to use a space-filling design such as Latin hypercube sampling (McKay, Beckman, Conover 1979), maximum entropy (Shewry and Wynn 1987), or maximum projection (Joseph, Gul, and Ba 2015). 

```{r}
#| label: latin-hypercube

set.seed(1303)
mlp_param %>%
  grid_space_filling(size = 20, original = FALSE) %>%
  ggplot(aes(x = .panel_x, y = .panel_y)) +
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) +
  labs(title = "Latin Hypercube design with 20 candidates")
```

### 13.2. Evaluating the Grid

```{r}
#| label: load-data

data(cells)
cells <- cells %>% select(-case)
```

```{r}
#| label: vfold_cv

set.seed(1304)
cell_folds <- vfold_cv(cells)
```

Predictors have a high degree of correlation, so use PCA to reduce dimensionality. Number of PCA components can be tuned. Lower-rank components have a wider range than the higher-rank compenents; normalization is needed. PCA is variance based and particularly susceptible to extreme values; a Yeo-Johnson transformation can be used to encourage more symmetric distributions. 

```{r}
#| label: recipe-workflow

mlp_rec <-
  recipe(class ~ ., data = cells) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_numeric_predictors(), num_comp = tune()) %>%
  step_normalize(all_numeric_predictors())

mlp_wflow <-
  workflow() %>%
  add_model(mlp_spec) %>%
  add_recipe(mlp_rec)

mlp_param <-
  mlp_wflow %>%
  extract_parameter_set_dials() %>%
  # epochs default too wide [10, 1000] and num_comp default too narrow [1, 4]
  update(
    epochs = epochs(c(50, 200)),
    # "0" components means no PCA transformation; used to compare with/without PCA
    num_comp = num_comp(c(0, 40))
  )
```

Tuning with grid search at 3 levels for each parameter (3 x 3 x 3 x 3 = 81 total combinations). Using ROC AUC on the 10-fold CV resample set to evaluate tune.

```{r}
#| label: tune-grid

roc_res <- metric_set(roc_auc)

set.seed(1305)
mlp_reg_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = mlp_param %>% grid_regular(levels = 3),
    metrics = roc_res
  )

mlp_reg_tune
```

Using high-level convenience functions to understand the results.

```{r}
#| label: understanding-results

autoplot(mlp_reg_tune) +
  scale_color_viridis_d(direction = -1) +
  theme(legend.position = "top")
```

Amount of penalization has the largest impact on ROC AUC. Epochs doesn't matter the match. Number of hidden units matters most when regularization is low (often harming performance). 

Can use *show_best()* to see the top results. Several param combos give similar results.

```{r}
#| label: show-best

show_best(mlp_reg_tune) %>% select(-.estimator)
```

Results indicate that another grid search using a larger weight decay penalty range should be explored. Can use a space-filling design (sfd) this time.

```{r}
#| label: space-filling-grid-search

set.seed(1306)
mlp_sfd_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = 20,
    # Pass in the parameter object to use the appropriate range:
    param_info = mlp_param,
    metrics = roc_res
  )

mlp_sfd_tune
```

Can still use *autoplot()* to visualize results, but works differently since the grid is not regular. Generates a marginal effects plot.

```{r}
#| label: autoplot-sfd

autoplot(mlp_sfd_tune)

show_best(mlp_sfd_tune) %>% select(-.estimator)
```

Smaller weight decay seems better, which is the opposite result from regular grid search. Each point in each panel is shared with the otehr 3 tuning params, i.e., the trends in one panel can be affected by te others. In regular grid search, each point in each panel is equally averaged over the other params, better isolating the trends. 

Often makes sense to choose a slightly suboptimal param combo if it's a simler model. In this case, larger penalty values and/or fewer hidden units.

Can use *extract* arg in *control_grid()* to retain fitted models and/or recipes, but usually no point. Similarly, can use *save_pred* arg in *collect_predictions()* to save predictions for each resample.

## 13.3 Finalizing the Model

Choosing final params.

```{r}
#| label: select-best

select_best(mlp_reg_tune, metric = "roc_auc")
```

Manually picking simpler model with only 1 hidden unit and higher penalty (basically penalized logistic regression).

```{r}
#| label: finalize-workflow

logistic_param <-
  tibble(
    num_comp = 0,
    epochs = 125,
    hidden_units = 1,
    penalty = 1
  )

final_mlp_wflow <-
  mlp_wflow %>%
  finalize_workflow(logistic_param)

final_mlp_wflow

final_mlp_fit <-
  final_mlp_wflow %>%
  fit(cells)
```

## 13.4. Tools for Creating Tuning Specifications

**usemodels** package provides high-level functions for quickly creating and fitting models with tuning specifications. User needs to add resample object and grid search details.

```{r}
#| label: usemodels-package

library(usemodels)

use_xgboost(
  Sale_Price ~ Neighborhood +
    Gr_Liv_Area +
    Year_Built +
    Bldg_Type +
    Latitude +
    Longitude,
  data = ames_train,
  # Add comments explaining some of the code:
  verbose = TRUE
)
```

## 13.5. Tools for Efficient Grid Search

### 13.5.1. Submodel Optimization

Some types of models can, from a single model fit, eval multiple tuning params without reffiting.

```{r}
#| label: submodel-optimization-example

c5_spec <-
  boost_tree(trees = tune()) %>%
  set_engine("C5.0") %>%
  set_mode("classification")

set.seed(1307)
c5_spec %>%
  tune_grid(
    class ~ .,
    resamples = cell_folds,
    grid = data.frame(trees = 1:100),
    metrics = roc_res
  )
```

Without the submodel optimization, the call to *tune_grid()* used 62.2 minutes to resample 100 submodels. With the optimization, the same call took 100 seconds (a 37-fold speed-up). The reduced time is the difference in *tune_grid()* fitting 1000 models versus 10 models.

### 13.5.2. Parallel Processing

When tuning via grid search, 2 distinct loops: resamples and tune param combos. Can parallelize either or both loops. **tune** package defaults to parallelizing over the resamples only.
________________
pseudo-code:

for (rs in resamples) {
  # Create analysis and assessment sets
  # Preprocess data (e.g. formula or recipe)
  for (mod in configurations) {
    # Fit model {mod} to the {rs} analysis set
    # Predict the {rs} assessment set
  }
}
_______________


Could combine loops over resamples and models into single loop.
_______________
pseudo-code:

all_tasks <- crossing(resamples, configurations)

for (iter in all_tasks) {                           
  # Create analysis and assessment sets for {iter}
  # Preprocess data (e.g. formula or recipe)
  # Fit model {iter} to the {iter} analysis set
  # Predict the {iter} assessment set
________________

In this case, parallelization now occurs over the single loop. For example, if we use 5-fold cross-validation with M tuning parameter values, the loop is executed over 5×M iterations. This increases the number of potential workers that can be used. However, the work related to data preprocessing is repeated multiple times. If those steps are expensive, this approach will be inefficient.

### 13.5.3. Benchmarking Boosted Trees

### 13.5.4. Access to Global Vars

When using parallel processing, the workers may not have access to all global variables defined in the main R session. Can use "!!" and "!!!" operators from **rlang** package to force evaluation of global variables before sending code to workers.

```{r}
#| label: global-vars-example

coef_penalty <- 0.1

spec <- linear_reg(penalty = !!coef_penalty) %>% set_engine("glmnet")
spec$args$penalty

mcmc_args <- list(chains = 3, iter = 1000, cores = 3)

linear_reg() %>% set_engine("stan", !!!mcmc_args)
```

Recommended when specifying model and recipe.

```{r}
#| label: global-vars-recipe

library(stringr)

ch_2_vars <- str_subset(names(cells), "ch_2")

# Still uses a reference to global data (~_~;)
recipe(class ~ ., data = cells) %>%
  step_spatialsign(all_of(ch_2_vars))

# Inserts the values into the step ヽ(•‿•)ノ
recipe(class ~ ., data = cells) %>%
  step_spatialsign(!!!ch_2_vars)
```

### 13.5.5. Racing Methods

Racing methods attempt to stop evaluating poor-performing tuning parameter combinations early in the resampling process.

```{r}
#| label: racing-methods

library(finetune)

set.seed(1308)
mlp_sfd_race <-
  mlp_wflow %>%
  tune_race_anova(
    cell_folds,
    grid = 20,
    param_info = mlp_param,
    metrics = roc_res,
    control = control_race(verbose_elim = TRUE)
  )

show_best(mlp_sfd_race, n = 10)
```

Other interim analysis techniques exists like traditional sequential analysis and Bradley-Terry.