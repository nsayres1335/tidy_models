---
title: "Tidy Modeling with R"
format: html
date: last-modified
execute:
    cache: true
code-fold: true
toc: true
---

## Intro

Code along with the [Tidy Modeling with R](https://www.tmwr.org/) book by Max Kuhn and Julia Silge.

## 4. Ames Housing Data

2,930 properties in Ames, Iowa.

```{r}
#| label: data

data(ames, package = "modeldata")
dim(ames)
```

74 columns covering house characteristics, locations, lot info, ratings, and sales prices.

```{r}
#| label: libraries
#| message: false

library(tidymodels)
tidymodels_prefer()
```

```{r}
#| label: distribution

ggplot(ames, aes(x = Sale_Price)) +
  geom_histogram(bins = 50)
```

Right skewed distribution. Let's log transform it.

```{r}
#| label: log-distribution

ggplot(ames, aes(x = Sale_Price)) +
  geom_histogram(bins = 50) +
  scale_x_log10()
```

More normal, better for inference.

```{r}
#| label: log-transform

ames <- ames |>
  mutate(Sale_Price = log10(Sale_Price))
```

Using log10 sale price from here on out.

## 5. Spending our Data

```{r}
#| label: set-seed

set.seed(501)
```

```{r}
#| label: split

ames_split <- initial_split(ames, prop = 0.8)
ames_split

ames_train <- training(ames_split)
ames_test <- testing(ames_split)

dim(ames_train)
```

There are `r dim(ames_train)[[1]]` and `r dim(ames_test)[[1]]` houses in the training and test sets. respectively.

```{r}
#| label: split-statrify

ames_split <- initial_split(ames, prop = 0.8, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

Stratified the split on `Sale_Price` since right skewed. Ensure similar distributions in training and test sets.

## 6. Fitting Model w/ Parsnip

```{r}
#| label: models

lm_model <- linear_reg() |>
  set_engine("lm")

lm_form_fit <- lm_model |>
  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)

lm_xy_fit <- lm_model |>
  fit_xy(
    x = ames_train |> select(Longitude, Latitude),
    y = ames_train |> pull(Sale_Price)
  )

lm_form_fit

lm_xy_fit
```

2 different ways to fit the same model: using a formula interface and using x/y interface.


```{r}
#| label: tidy

tidy(lm_form_fit)
```

tidy() in place of summary() for tidy results.

```{r}
#| label: predict

ames_test_small <- ames_test |> slice(1:5)

predict(lm_form_fit, new_data = ames_test_small)
```

Predicting on a subset of the test data.

```{r}
#| label: bind_pred

ames_test_small |>
  select(Sale_Price) |>
  bind_cols(predict(lm_form_fit, new_data = ames_test_small)) |>
  bind_cols(predict(lm_form_fit, ames_test_small, type = "pred_int"))
```

Binding predictions and prediction intervals to the actual sale prices.


```{r}
#| label: tree

tree_model <-
  decision_tree(min_n = 2) %>%
  set_engine("rpart") %>%
  set_mode("regression")

tree_fit <-
  tree_model %>%
  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)

ames_test_small %>%
  select(Sale_Price) %>%
  bind_cols(predict(tree_fit, ames_test_small))
```

Example of using a decision tree model instead of linear regression.

parsnip can support a lot more models ([see](https://parsnip.tidymodels.org/find/)).

Can use **parsnip_addin()** in RStudio to view list of poissible models for each model mode. Unfortunately however, Positron does not currently support RStudio addins.

## 7. Workflow Basics

Workflows help bundle pre-processing, fit, and post-proc steps.

```{r}
#| label: workflow-setup

lm_model <- linear_reg() |>
  set_engine("lm")

lm_wflow <- workflow() |>
  add_model(lm_model) |>
  add_formula(Sale_Price ~ Longitude + Latitude)
```

Adding model and pre-processing (formula) to the workflow.

```{r}
#| label: workflow-fit

lm_fit <- fit(lm_wflow, ames_train)
lm_fit
```

Workflows have a fit() method that cas be used to fit the model.


```{r}
#| label: workflow-predict

predict(lm_fit, ames_test |> slice(1:5))
```

Can also use predict() method on the fitted workflow.

```{r}
#| label: workflow-update

lm_fit |> update_formula(Sale_Price ~ Longitude)
```

Can update the workflow to change the formula or model.

```{r}
#| label: workflow-add-vars

lm_wflow <- lm_wflow |>
  remove_formula() |>
  add_variables(outcomes = Sale_Price, predictors = c(Longitude, Latitude))

lm_wflow
```

Can also add variables instead of a formula. Works w/ dplyr grammar. Facilitates more complex moedeling specs. Particularly useful w/ models such as glmnet and xgboost which expect user to make indicator vars from factor preds.

```{r}
#| label: handling-special-multilevel
#| eval: false

# install.packages("multilevelmod")
library(multilevelmod)

multilevel_spec <- linear_reg() %>% set_engine("lmer")

multilevel_workflow <-
  workflow() %>%
  # Pass the data along as-is:
  add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %>%
  add_model(
    multilevel_spec,
    # This formula is given to the model
    formula = distance ~ Sex + (age | Subject)
  )

multilevel_fit <- fit(multilevel_workflow, data = Orthodont)
multilevel_fit
```

Standard R methods can't properly process this formula. Workflows handles by using optional suplementary model forumla argument in add_model(). add_variable() specifies bare col names; add_model() takes the actual formula given to the model.

```{r}
#| label: handling-special-survival

# install.packages("censored")
library(censored)

parametric_spec <- survival_reg()

parametric_workflow <-
  workflow() %>%
  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %>%
  add_model(parametric_spec, formula = Surv(futime, fustat) ~ age + strata(rx))

parametric_fit <- fit(parametric_workflow, data = ovarian)
parametric_fit
```

Another example using a survival analysis model.

```{r}
#| label: workflowsets-setup

library(workflowsets)

location <- list(
  longitude = Sale_Price ~ Longitude,
  latitude = Sale_Price ~ Latitude,
  coords = Sale_Price ~ Longitude + Latitude,
  neighborhood = Sale_Price ~ Neighborhood
)

location_models <-
  workflow_set(
    preproc = location,
    models = list(lm = lm_model)
  )

location_models

location_models$info[[1]]

extract_workflow(location_models, id = "coords_lm")
```

workflowsets package to create a set of workflows to compare different pre-processing and model combinations. Useful for finding best predictive models or assessing predictors.

```{r}
#| label: workflowsets-fit

location_models <- location_models |>
  mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))

location_models

location_models$fit[[1]]
```

Iterating info column to fit each workflow and mutating onto existing tibble for tidyness.

```{r}
#| label: eval-test-set

final_lm_res <- last_fit(lm_wflow, ames_split)
final_lm_res

fitted_lm_wflow <- extract_workflow(final_lm_res)

collect_metrics(final_lm_res)
collect_predictions(final_lm_res) %>% slice(1:5)
```

**last_fit()** convenience function to fit final model to entire training set and evaluate on the test set in one step. Returns a tibble with the fitted workflow, metrics, and predictions. Each have helper functions to extract/collect them.

## 8. FE w/ Recipes

* neighborhood: qual, 29 unique in training set 
* gross-above grade living area (Gr_Liv_Area): continuous 
* Year built (Year_Built)
* Type of building (Bldg_Type): OneFam (n = 1936), TwoFmCon (n = 50), Duplex (n = 88), Twnhs (n = 77), TwnhsE (n = 191)


```{r}
#| label: recipe-setup

library(tidymodels) # Includes the recipes package
tidymodels_prefer()

simple_ames <-
  recipe(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
    data = ames_train
  ) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_dummy(all_nominal_predictors())

simple_ames
```

Setting up a recipe to log transform `Gr_Liv_Area` and create dummy variables for categorical predictors.

```{r}
#| label: recipe-workflow

lm_wflow <-
  lm_wflow |>
  remove_variables() |> # can only have one pre-proc method at a time
  add_recipe(simple_ames)

lm_wflow
```

Updating the workflow to use the recipe pre-proc workflow.

```{r}
#| label: recipe-fit

lm_fit <- fit(lm_wflow, ames_train)

predict(lm_fit, ames_test |> slice(1:3))
```

Fitting the workflow with the recipe and making predictions.

```{r}
#| label: recipe-extract-tidy

lm_fit |> extract_recipe(estimated = T)

lm_fit |>
  extract_fit_parsnip() |>
  tidy() |>
  slice(1:5)
```

Can extract recipes and objects then tidy.

```{r}
#| label: step-other

simple_ames <-
  recipe(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
    data = ames_train
  ) |>
  step_log(Gr_Liv_Area, base = 10) |>
  step_other(Neighborhood, threshold = 0.01) |>
  step_dummy(all_nominal_predictors())
```

Using **step_other()** to lump infrequent levels of `Neighborhood` into "other" category. Levels that make up less than 1% of the data.

```{r}
#| label: fig-interaction-terms

ggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = .2) +
  facet_wrap(~Bldg_Type) +
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Gross Living Area", y = "Sale Price (USD)")
```

The effect of building type on sale price seems to effect (AKA interact) with gross living area.

```{r}
#| label: step-interact-formula

Sale_Price ~ Neighborhood +
  log10(Gr_Liv_Area) +
  Bldg_Type +
  log10(Gr_Liv_Area):Bldg_Type
# or
Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type
```

In a formula interactions can be specified using ":" or "*".

```{r}
#| label: step-interact-recipe

simple_ames <-
  recipe(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
    data = ames_train
  ) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_other(Neighborhood, threshold = 0.01) %>%
  step_dummy(all_nominal_predictors()) %>%
  # Gr_Liv_Area is on the log scale from a previous step
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_"))
```

```{r}
#| label: splines

# install.packages("patchwork")
library(patchwork)
library(splines)

plot_smoother <- function(deg_free) {
  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) +
    geom_point(alpha = .2) +
    scale_y_log10() +
    geom_smooth(
      method = lm,
      formula = y ~ ns(x, df = deg_free),
      color = "lightblue",
      se = FALSE
    ) +
    labs(title = paste(deg_free, "Spline Terms"), y = "Sale Price (USD)")
}

(plot_smoother(2) + plot_smoother(5)) / (plot_smoother(20) + plot_smoother(100))
```

2 terms underfit, while 100 terms overfit. 5 or 20 terms seem reasonable.

```{r}
#| label: splines-recipe

ames_rec <-
  recipe(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,
    data = ames_train
  ) |>
  step_log(Gr_Liv_Area, base = 10) |>
  # can specify id ahead of time
  step_other(Neighborhood, threshold = 0.01, id = "my_id") |>
  step_dummy(all_nominal_predictors()) |>
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) |>
  step_ns(Latitude, deg_free = 20)

ames_rec
```

Using **step_ns()** to create natural spline basis functions for `Latitude` with 20 degrees of freedom.

```{r}
#| label: recipe-tidy

tidy(ames_rec)
```

Recipes can also be tidied to view details of each step.

```{r}
#| label: refit-workflow

lm_wflow <-
  workflow() %>%
  add_model(lm_model) %>%
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

Refiting the workflow with the updated recipe.

```{r}
#| label: tidy-my-id-recipe

estimated_recipe <-
  lm_fit %>%
  extract_recipe(estimated = TRUE)

# w/ id
tidy(estimated_recipe, id = "my_id")

# or w/ index
tidy(estimated_recipe, number = 2)
```

The **tidy()** method can be called along w/ *id* to get results of **step_other()**.

