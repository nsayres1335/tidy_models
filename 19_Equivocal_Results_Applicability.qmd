---
title: "Chapter 19. When Should You Trust Your Predictions?"
format: html
date: last-modified
execute:
    cache: true
code-fold: true
toc: true
---

## 19.1. Equivocal Results

Simulating classification data with two classes and two predictors (x and y). Preds follow a bivariate normal dist w/ corr of 0.7.

```{r}
#| label:

library(tidymodels)
tidymodels_prefer()

simulate_two_classes <-
  function(n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2)) {
    # Slightly correlated predictors
    sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)
    dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)
    colnames(dat) <- c("x", "y")
    cls <- paste0("class_", 1:2)
    dat <-
      as_tibble(dat) %>%
      mutate(
        linear_pred = !!eqn,
        # Add some misclassification noise
        linear_pred = linear_pred + rnorm(n, sd = error),
        prob = binomial()$linkinv(linear_pred),
        class = ifelse(prob > runif(n), cls[1], cls[2]),
        class = factor(class, levels = cls)
      )
    dplyr::select(dat, x, y, class)
  }

set.seed(1901)
training_set <- simulate_two_classes(200)
testing_set <- simulate_two_classes(50)
```

Estimate a logistic reg model using Bayesian methods:

```{r}
#| label: log-reg-fit

two_class_mod <-
  logistic_reg() %>%
  set_engine("stan", seed = 1902) %>%
  fit(class ~ . + I(x^2) + I(y^2), data = training_set)

print(two_class_mod, digits = 3)
```

Creating preds:

```{r}
#| label: log-reg-preds

test_pred <- augment(two_class_mod, testing_set)
test_pred %>% head()
```

Using **probably** package for equivocal zones:

```{r}
library(probably)

lvls <- levels(training_set$class)

test_pred <-
  test_pred %>%
  mutate(
    .pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15)
  )

test_pred %>% count(.pred_with_eqz)
```

Rows that are within 0.50 Â± 0.15 are fiven a value of `[EQ]`. The notation is nota factor level, but an attribute of that column.

Can use confusion matrices and other stats to compare, since pred factors levels are the same as original data. Can also use `is_equivocal()` to filter these rows.

```{r}
test_pred %>% conf_mat(class, .pred_class)

test_pred %>% conf_mat(class, .pred_with_eqz)
```

Does the equivocal zone improve accuracy?

```{r}
# A function to change the buffer then compute performance.
eq_zone_results <- function(buffer) {
  test_pred <-
    test_pred %>%
    mutate(
      .pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer)
    )
  acc <- test_pred %>% accuracy(class, .pred_with_eqz)
  rep_rate <- reportable_rate(test_pred$.pred_with_eqz)
  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)
}

# Evaluate a sequence of buffers and plot the results.
map(seq(0, .1, length.out = 40), eq_zone_results) %>%
  list_rbind() %>%
  pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") %>%
  ggplot(aes(x = buffer, y = value, lty = statistic)) +
  geom_step(linewidth = 1.2, alpha = 0.8) +
  labs(y = NULL, lty = NULL)
```

The accuracy improves by a few percentage points at the cost of 10% of predictions being unusable. The value of this compromise depends on the application.

Can use standard error of the class probability instead of the predicted class probabilty. Since we used a Bayesian model, the prob estimates are actually the mean of the posterior predictive distribution, giving us a distribution for the class probability. Can use to get a standard error of prediction of the probability. Can get upper and lower limits too:

```{r}
test_pred <-
  test_pred %>%
  bind_cols(
    predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
  )

test_pred
```

Can be used on numeric predictions, but doesn't always work.

## 19.2. Determing Model Applicability

From **modeldata** package, Chicago ridership data set. Contains daily values between 01/22/2001 and 08/28/2016. Get the last 14 days as a test set:

```{r}
## loads both `Chicago` data set as well as `stations`
data(Chicago)

Chicago <- Chicago %>% select(ridership, date, one_of(stations))

n <- nrow(Chicago)

Chicago_train <- Chicago %>% slice(1:(n - 14))
Chicago_test <- Chicago %>% slice((n - 13):n)
```

Main preds are the date and lagged ridership data at different stations. Predictors are highly correlated; using PLS.

```{r}
base_recipe <-
  recipe(ridership ~ ., data = Chicago_train) %>%
  # Create date features
  step_date(date) %>%
  step_holiday(date, keep_original_cols = FALSE) %>%
  # Create dummy variables from factor columns
  step_dummy(all_nominal()) %>%
  # Remove any columns with a single unique value
  step_zv(all_predictors()) %>%
  step_normalize(!!!stations) %>%
  step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))

lm_spec <-
  linear_reg() %>%
  set_engine("lm")

lm_wflow <-
  workflow() %>%
  add_recipe(base_recipe) %>%
  add_model(lm_spec)

set.seed(1902)
lm_fit <- fit(lm_wflow, data = Chicago_train)
```

Assessing fit:

```{r}
res_test <-
  predict(lm_fit, Chicago_test) %>%
  bind_cols(
    predict(lm_fit, Chicago_test, type = "pred_int"),
    Chicago_test
  )

res_test %>% select(date, ridership, starts_with(".pred"))

res_test %>% rmse(ridership, .pred)
```

Can use **applicable** package to develop an applicability domain for the model.

```{r}
library(applicable)
pca_stat <- apd_pca(
  ~.,
  data = Chicago_train %>% select(one_of(stations)),
  threshold = 0.99
)
pca_stat

autoplot(pca_stat, distance) + labs(x = "distance")
```

Computing percentile for new data:

```{r}
score(pca_stat, Chicago_test) %>% select(starts_with("distance"))
```

Pretty reasonable.

Could do PCA on PLS scores instead of the individual predictor columns. 